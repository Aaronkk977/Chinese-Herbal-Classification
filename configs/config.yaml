# Configuration for Chinese Herbal Classification
# Based on parameters from the research paper

data:
  dataset_path: "data"  # Path to properly split dataset (train/val/test)
  image_size: 224
  num_classes: 100  # Will be auto-detected from dataset
  num_workers: 4

preprocessing:
  # Normalization (ImageNet statistics for pretrained models)
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  # Grayscale conversion (as per paper: weighted average method)
  # Helps reduce ad/watermark noise by focusing on texture instead of color
  # Output is still 3-channel (R=G=B) for pretrained model compatibility
  use_grayscale: false
  # Denoising with median filter (kernel must be odd integer >= 3)
  median_filter_kernel: 5
  
augmentation:
  # Data augmentation techniques
  rotation_degrees: 30
  horizontal_flip: true
  vertical_flip: true
  random_crop: true
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2

model:
  name: "ConvNeXt-Tiny-ACMix"
  backbone: "convnext_tiny"
  num_acmix_blocks: 2  # K=2 in paper (best performance)
  network_width: 768
  num_layers: 22  # Best depth from paper
  activation: "gelu"
  drop_path_rate: 0.1
  pretrained: true

training:
  epochs: 100
  batch_size: 8
  learning_rate: 0.0001  # As per paper (0.001 causes NaN)
  optimizer: "adamw"
  weight_decay: 0.0001
  scheduler: "cosine"
  warmup_epochs: 5
  label_smoothing: 0.1
  gradient_clip: 1.0  # Gradient clipping to prevent exploding gradients
  
  # Mixed precision training
  use_amp: true
  
  # Checkpointing
  save_freq: 10
  early_stopping_patience: 10

hardware:
  device: "cuda"  # Always use GPU
  gpu_id: 1       # GPU ID to use (0, 1, 2, etc.) (can override with --gpu-id)
  seed: 42

paths:
  checkpoint_dir: "../checkpoints"
  results_dir: "../results"
  log_dir: "../logs"
